from fastapi import FastAPI, HTTPException
from pathlib import Path
from app.utils.time_series import load_energy_consumption_data ,save_data_to_influxdb
from app.services.prediction_service import apply_arima_model,save_predictions_to_postgres,get_influx_data,connect_postgresql
import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
from pydantic import BaseModel
import os
from app.services.enrich_data import add_time_features,load_data_from_postgres,save_to_influx
from app.config.config import INFLUX_URL, INFLUX_ORG,INFLUX_TOKEN,INFLUX_BUCKET
from influxdb_client import InfluxDBClient
import logging
#from app.services.lstm_model import get_influxdb_client,load_data_from_influx,prepare_data_for_lstm,train_lstm


#from app.services.lstm_model import get_influxdb_client,load_data_from_influx,train_lstm,prepare_data_for_lstm


app = FastAPI()


# D√©finir le chemin du fichier de donn√©es
DATASET_DIR = Path("D:/PFE/DataSet")
FILE_CSV = DATASET_DIR / "Energy_consumption.csv"
data_cache = None  # Stocke les donn√©es apr√®s /load-data

@app.get("/")
def root():
    return {"message": "Bienvenue dans le service de pr√©diction"}


@app.get("/load-data")
def load_data():
    global data_cache  # Permet de modifier la variable globale

    try:
        print("üìÇ V√©rification de l'existence du fichier CSV...")
        if not os.path.exists(str(FILE_CSV)):  # V√©rifier si le fichier existe
            raise HTTPException(status_code=404, detail="Fichier introuvable. V√©rifiez le chemin.")

        # Charger les donn√©es et r√©cup√©rer le nombre de lignes
        df, nombre_de_lignes = load_energy_consumption_data(str(FILE_CSV))

        print(f"‚úÖ {nombre_de_lignes} lignes charg√©es apr√®s nettoyage.")
        print("üîç V√©rification des premi√®res lignes du DataFrame...")
        print(df.head())

        # V√©rification des colonnes n√©cessaires
        if not all(col in df.columns for col in ['Year', 'Month', 'Day', 'Hour']):
            raise HTTPException(status_code=400, detail="Erreur : Les colonnes 'Year', 'Month', 'Day', 'Hour' sont manquantes.")

        # Reconstituer la colonne 'Timestamp'
        df['Timestamp'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour']], errors='coerce')

        if 'Timestamp' not in df.columns or df['Timestamp'].isnull().all():
            raise HTTPException(status_code=400, detail="Erreur : La colonne 'Timestamp' est absente ou contient des valeurs invalides.")

        # Enregistrer les donn√©es
        data_cache = df

        # Sauvegarder dans InfluxDB
        save_data_to_influxdb(df)

        # Retourner les donn√©es sous forme de dictionnaire avec le nombre de lignes
        data_dict = df[['Timestamp', 'Temperature', 'Humidity', 'SquareFootage', 'Occupancy', 'RenewableEnergy', 'EnergyConsumption']].head().to_dict(orient="records")
        return {"nombre_de_lignes": nombre_de_lignes, "data": data_dict}

    except Exception as e:
        print(f"‚ùå Exception captur√©e : {e}")
        raise HTTPException(status_code=500, detail=f"Erreur lors du chargement des donn√©es : {e}")
    




# Configuration du logging
logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")

@app.get("/forecast")
def forecast_data():
    """
    Route pour effectuer des pr√©visions et les enregistrer dans PostgreSQL.
    """
    global data_cache  # Supposons que les donn√©es sont d√©j√† charg√©es

    if data_cache is None:
        raise HTTPException(status_code=400, detail="Les donn√©es doivent d'abord √™tre charg√©es via /load-data.")

    # Nettoyage des colonnes
    data_cache.columns = data_cache.columns.str.strip()
    logging.debug(f"Noms des colonnes apr√®s nettoyage : {list(data_cache.columns)}")

    # Trouver la colonne 'energyConsumption'
    column_name = next((col for col in data_cache.columns if col.lower() == "energyconsumption"), None)

    if column_name is None:
        logging.error("Erreur : la colonne 'energyConsumption' est absente des donn√©es.")
        raise HTTPException(status_code=400, detail="Les donn√©es doivent contenir la colonne 'energyConsumption'.")

    if data_cache[column_name].isnull().any():
        logging.error("Erreur : La colonne 'energyConsumption' contient des valeurs manquantes.")
        raise HTTPException(status_code=400, detail="La colonne 'energyConsumption' contient des valeurs manquantes.")

    try:
        # Appliquer ARIMA pour g√©n√©rer des pr√©visions
        forecast_df = apply_arima_model(data_cache.rename(columns={column_name: "energyConsumption"}), steps=120)
        logging.info(f"Nombre de pr√©dictions g√©n√©r√©es : {len(forecast_df)}")

        # Renommer les colonnes pour correspondre √† la base de donn√©es
        forecast_df = forecast_df.rename(columns={"date": "Timestamp", "predicted": "Forecast"})

        # Sauvegarder dans PostgreSQL
        save_predictions_to_postgres(forecast_df)

        return {"message": "Pr√©visions g√©n√©r√©es et enregistr√©es avec succ√®s.", "forecast": forecast_df.tail(10).to_dict(orient="records")}

    except Exception as e:
        logging.exception("Erreur lors de la g√©n√©ration des pr√©visions")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la g√©n√©ration des pr√©visions : {e}")







class PredictionRequest(BaseModel):
    steps: int = 30

@app.post("/predict")
async def predict(request: PredictionRequest):
    """Endpoint pour r√©cup√©rer les donn√©es, faire une pr√©diction et enregistrer les r√©sultats."""
    
    # √âtape 1 : Charger les donn√©es depuis InfluxDB
    data = get_influx_data()
    
    print("Donn√©es r√©cup√©r√©es :", data)  # Ajout pour voir les donn√©es
    
    # V√©rification
    if "energyConsumption" not in data.columns:
        raise ValueError("Les donn√©es r√©cup√©r√©es ne contiennent pas 'energyConsumption'.")

    # √âtape 2 : Appliquer le mod√®le ARIMA pour pr√©dire
    forecast_df = apply_arima_model(data, steps=request.steps)

    # √âtape 3 : Enregistrer les pr√©visions dans PostgreSQL
    save_predictions_to_postgres(forecast_df)

    return {"message": "Les pr√©visions ont √©t√© g√©n√©r√©es et enregistr√©es avec succ√®s."}


#Vlaue in  Postgres 
@app.get("/predictions")
async def get_predictions():
    """Endpoint pour r√©cup√©rer les pr√©visions enregistr√©es dans PostgreSQL"""
    conn = connect_postgresql()
    cursor = conn.cursor()
    
    cursor.execute("SELECT timestamp, forecast FROM predictions ORDER BY timestamp DESC")
    data = cursor.fetchall()

    cursor.close()
    conn.close()

    if not data:
        return {"message": "Aucune pr√©vision disponible."}

    predictions = [{"timestamp": row[0], "forecast": row[1]} for row in data]
    return {"predictions": predictions}




@app.get("/sync-postgres-to-influx")
def sync_postgres_to_influx():
    """
    Endpoint pour synchroniser les donn√©es de PostgreSQL vers InfluxDB.
    """
    try:
        # Charger les donn√©es depuis PostgreSQL
        df = load_data_from_postgres()

        if df.empty:
            raise HTTPException(status_code=404, detail="Aucune donn√©e trouv√©e dans PostgreSQL.")

        # Ajouter les variables temporelles
        df = add_time_features(df)

        # Afficher le nombre total de lignes apr√®s l'ajout des variables temporelles
        logging.debug(f"Nombre total de lignes apr√®s ajout des variables temporelles : {len(df)}")

        # Sauvegarder les donn√©es dans InfluxDB
        save_to_influx(df)

        # Retourner un aper√ßu des nouvelles donn√©es
        return {
            "message": "Donn√©es synchronis√©es avec succ√®s.",
            "preview": df.head().to_dict(orient="records"),
            "total_rows": len(df)  # Ajouter le nombre total de lignes trait√©es
        }

    except Exception as e:
        logging.error(f"Erreur lors de la synchronisation : {e}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la synchronisation : {e}")


@app.get("/get-influx-data")
def get_influx_data():
    """
    Endpoint pour r√©cup√©rer les donn√©es enregistr√©es dans InfluxDB et compter le nombre de lignes.
    """
    try:
        client = InfluxDBClient(url=INFLUX_URL, token=INFLUX_TOKEN, org=INFLUX_ORG)
        query_api = client.query_api()

        query = f'''
        from(bucket: "{INFLUX_BUCKET}")
          |> range(start: 0)
          |> filter(fn: (r) => r._measurement == "energy_data")
        '''
        tables = query_api.query(query, org=INFLUX_ORG)

        results = []
        for table in tables:
            for record in table.records:
                results.append({
                    "timestamp": record.get_time(),
                    "forecast": record.get_value(),
                    "field": record.get_field()
                })

        client.close()

        nombre_de_lignes = len(results)  # ‚úÖ Nombre total de lignes r√©cup√©r√©es

        if not results:
            return {"message": "Aucune donn√©e trouv√©e dans InfluxDB.", "nombre_de_lignes": 0}

        return {"nombre_de_lignes": nombre_de_lignes, "data": results}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es InfluxDB : {e}")



